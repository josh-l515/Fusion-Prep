# 1.8 Distribución de Gauss
Consideremos de nuevo la serie de resultados de medición
```{math}
x_1, x_2, x_3, \dotsb, x_i, \dotsb, x_{N-1}, x_N
```
estos números están distribuidos alrededor del promedio $\bar{x}$. Observaremos en {numref}`fig_gauss_01` que hay valores que están cerca del promedio; otros, menores en número, estarán lejos. 
```{figure} _static/fig_gauss_01.png
:alt: distribución de mediciones normal
:width: 700px
:align: center
:name: fig_gauss_01
Distribución normal de mediciones repetidas de una misma magnitud
```
Cuando nos proponemos hacer una medición más, la $(N+1)-$ésima, no podemos saber de antemano el resultado que va a salir, pero es evidente que sí podremos decir que con buena probabilidad estará cerca del promedio, y con probabilidad menor estará lejos, esto es:

> **No podremos nunca predecir el valor de una medición dada, pero sí podremos decir algo sobre la probabilidad de que su valor caiga en un determinado intervalo de valores posibles**

Si dividimos el eje $x$ en pequeños intervalos iguales $\Delta x$ , podemos contar el número de observaciones $\Delta n$ que caen en cada intervalo y representarlo gráficamente.
```{figure} _static/fig_gauss_02.png
:alt: distribución de gauss mediciones fisicas
:width: 600px
:align: center
:name: fig_gauss_02
Esto es lo que se llama un histograma, cuando a todo un intervalo le corresponde un valor, y no a un solo punto, como sucede en una función. 
```
Cuanto más grande sea la estadística, o sea $N$, más pequeños podemos hacer los intervalos $\Delta x$ sin por ello perder la chance de tener un número suficientemente grande $\Delta n$ de datos en cada intervalo. 

Sea $\Delta n$ el número de valores numéricos de nuestra serie de mediciones, que caen en un determinado intervalo, entre $x$ y $x + \Delta x$. Se comprueba experimentalmente que ese número depende del valor de $x$ y de la longitud del intervalo $\Delta x$ en la forma aproximada:
```{math}
\Delta n \approx \frac{N}{\sigma \sqrt{2\pi}}e^{-\displaystyle{\frac{(\bar{x}-x)^2}{2\sigma^2}}}\Delta x
```
La aproximación es tanto mejor cuanto mayor sea $N$ y cuanto más pequeño sea $\Delta x$. La relación se transforma en igualdad para diferenciales $dn$ y $dx$.

```{math}
\lim_{\Delta x\to 0}\frac{\Delta n}{\Delta x} =\frac{dn}{dx}= \frac{N}{\sigma \sqrt{2\pi}}e^{-\displaystyle{\frac{(\bar{x}-x)^2}{2\sigma^2}}}
```

donde: 
  - $\displaystyle{\frac{dn}{dx}}$: se conoce como densidad de oservaciones, es decir representa el número de observaciones cuyos valores estan comprendidos entre $x$ y $x+dx$.
  - $x$: ubica el intervalo $dx$ en el cual se tienen $dn$ observaciones.

La representación gráfica de la densidad de observaciones se llama curva de distribución o curva de Gauss. Tiene la forma de {numref}`fig_gauss_03`

```{figure} _static/fig_gauss_03.png
:alt: curva de gauss
:width: 600px
:align: center
:name: fig_gauss_03
Curva de Gauss
```
donde la curva: 
  - $x=\bar{x}$ es un máximo 
  - Es simétrica respecto de $\bar{x}$.
  - Tiene forma de campana.
  - Sus punto de inflexión están en $\bar{x}\pm \sigma$. 
  - Tiende a cero cuando nos alejamos de $\bar{x}$. 

## 1.8.1 Número de observaciones
La superficie total subtendida por la curva de Gauss, es:
```{math}
\int_{\infty}^\infty dn&=\int_{-\infty}^\infty \frac{N}{\sigma \sqrt{2\pi}}e^{-\displaystyle{\frac{(\bar{x}-x)^2}{2\sigma^2}}}dx\\
&=\frac{N}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{-\displaystyle{\frac{(\bar{x}-x)^2}{2\sigma^2}}}\frac{dx}{\sigma}\\
&=\frac{N}{\sqrt{2\pi}}\sqrt{2\pi}\\
&=N
```
EL cual corresponde al número total de observaciones. Esta es precisamente la razón por la cual se coloca el factor $\sqrt{2\pi}$ en el denominador.

Integremos ahora sobre cierto intervalo respecto a $x$, la integral:
```{math}
:label: eq_numero_observaciones
\int_{x_1}^{x_2} dn&=\int_{x_1}^{x_2} \frac{N}{\sigma \sqrt{2\pi}}e^{-\displaystyle{\frac{(\bar{x}-x)^2}{2\sigma^2}}}dx\\
&=\int_{x_1}^{x_2} \frac{dN}{dx}dx\\
&=\Delta N
```
será el número de observaciones $\Delta N$ comprendidas entre $x_1$ y $x_2$.  Esta integral no puede resolverse de forma analítica, su valor debe buscarse en tablas provenientes de un cálculo numérico.

## 1.8.2 Probabilidad de observación
Si ahora dividimos {eq}`eq_numero_observaciones`, por el número total de observaciones $N$, 
```{math}
\frac{\Delta N}{N}= \frac{1}{\sigma \sqrt{2\pi}}\int_{x_1}^{x_2}e^{-\displaystyle{\frac{(\bar{x}-x)^2}{2\sigma^2}}}dx
```
obtenemos lo que se llama la probabilidad de que una observación dada esté comprendida en el intervalo $x_1$, $x_2$, así podemos decir que:
- La probabilidad de encontrar un dato entre $-\infty$ y $+\infty$ es 1 o sea $100\%$ certeza. 
- La probabilidad de encontrar un dato fuera del intervalo $x_1$, $x_2$, será 
    ```{math}
  1-\frac{\Delta N}{N}
  ```
> En resumen, de esto deducimos que, si bien es imposible predecir el valor exacto que saldrá de una medición dada, sí podemos decir algo sobre la probabilidad de que ese valor esté comprendido en un intervalo dado. La predicción de estas probabilidades es la utilidad fundamental de la función de Gauss.
## 1.8.3 Tabla de la distribución normal
Veamos las probabilidades para algunos intervalos "prototipo" dentro de las mediciones experimentalmente. La probabilidad de que el valor de una medición (no el valor verdadero) caiga entre:
  ```{math}
 \bar{x}\pm\sigma& \to  68.3\%\\
 \bar{x}\pm2\sigma& \to 95.4\%\\
 \bar{x}\pm3\sigma& \to 99.7\%\\
 \bar{x}\pm4\sigma& \to 99.994\%\\
 \bar{x}\pm5\sigma& \to 99.99994\%

```
Estas cifras se obtienen de tablas de la función de densidad acumulada de la distribución normal estándar

```{figure} _static/tablaNormal.png
:alt: Tabla z
:width: 800px
:align: center
:name: fig_tabla_z
Tabla z de probabilidad distribución normal
```
### ¿Qué es la variable $z$?

La variable $z$ se llama **variable aleatoria estándar normal** y se define como:

```{math}
z = \frac{x - \mu}{\sigma}
```
$z$ es una **transformación lineal** que convierte cualquier variable aleatoria $x$, con distribución normal de media $\mu$ y desviación estándar $\sigma$, en una variable con:

- Media $ \mu=0$  
- Desviación estándar $\sigma=1$

Esta transformación se llama **normalización** o **estandarización**. Para demostrar esto, partamos de la distribución normal definida por su función de densidad de probabilidad:
```{math}
f_X(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{\displaystyle{-\frac{(x-\mu)^2}{2\sigma^2}}}
```
Cuando hacemos el cambio de variable:
```{math}
z = \frac{x - \mu}{\sigma} \quad \text{es decir,} \quad x = \mu + \sigma z
```
Usamos la fórmula para el cambio de variable en distribuciones continuas:

```{math}
f_Z(z) = f_X(x) \cdot \left| \frac{dx}{dz} \right|
```

Dado que:

```{math}
\frac{dx}{dz} = \sigma
```

Entonces:

```{math}
f_Z(z) = f_X(\mu + \sigma z) \cdot \sigma
```

Sustituyendo $f_X$:

```{math}
f_Z(z) &= \frac{1}{\sigma \sqrt{2\pi}} e^{-\displaystyle{\frac{(\mu + \sigma z - \mu)^2}{2\sigma^2}}} \cdot \sigma\\
&= \frac{1}{\sqrt{2\pi}}e^{-\displaystyle{\frac{z^2}{2}}}
```
### Resultado:
Finalmente, los valores de la función de densidad cuya variable $z$ aparece en la tabla de probabilidad acumulada, es
```{math}
f_Z(z) = \frac{1}{\sqrt{2\pi}} e^{\displaystyle{-\frac{z^2}{2}}}
```
Es decir,  esta es la **distribución normal estándar**, con media $\mu=0$ y desviación estándar $\sigma=1$.


> Como existen infinitas distribuciones normales (cada una con distintos valores de $\mu$ y $\sigma$), sería poco práctico tener una tabla para cada caso. La estandarización permite usar una **única tabla** para consultar probabilidades o percentiles, como la **tabla $z$**. Gracias a esta transformación, cualquier valor de $x$ en una normal puede expresarse como un valor de $z$. Así podemos consultar probabilidades acumuladas de forma directa.

---


## 1.8.4 Ejercicios de Aplicación

1. De los siguientes datos medidos en laboratorio

    ```{figure} _static/tabla_ejercicio.png
    :alt: errores sistematicos
    :width: 600px
    :align: center
    :name: fig_errores_sistematicos
    Datos experimentales ejercicio 01
    ```

    Calcular:
      - La media aritmética $\bar{t}$, la desviación estándar $\sigma$ de cada una de las medidas y la incertidumbre estandar o típica $\xi$. 
      -  Reporta el resultado final en el formato (Considerando la incertidumbre tipo A):
          ```{math}
          t=\bar{t} \pm \xi_A \quad [^\circ\text{C}]
          ```
      - Considera que las mediciones se realizaron con un termómetro digital cuya resolución minima fue de $\delta_t=\xi_B= 0.01\, ^\circ\rm C$, reporta el resultado final, considerando la incertidumbre tipo B ($\xi_B$)
          ```{math}
          t=\bar{t} \pm \xi_{\rm total} \quad [^\circ\text{C}]
          ```
      - Elabora un **histograma** con los $20$ valores de temperatura.
      - Superpón (en la misma gráfica) una **curva gaussiana teórica** con media $\bar{t}$ y desviación estándar  $\sigma$
      - Asumiendo una **distribución normal**, calcula  La probabilidad de que una futura medida de temperatura:
        - sea menor a $97.5 ^\circ\rm C $
        - sea menor a $100.0 ^\circ\rm C$
        - se encuentre entre $99.5 ^\circ\rm C$ y $100.5 ^\circ\rm C$
        - sea mayor a $98.00 ^\circ\rm C$

